{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bbat54/Bbat54/blob/main/XG_Boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26a64e43",
      "metadata": {
        "id": "26a64e43"
      },
      "source": [
        "# XGBoost model exploration\n",
        "### Batchimeg Battur"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ceadb0",
      "metadata": {
        "id": "60ceadb0"
      },
      "source": [
        "##### Import the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ddb2c9e",
      "metadata": {
        "id": "5ddb2c9e"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "import io\n",
        "from google.colab import files\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca2c083a",
      "metadata": {
        "id": "ca2c083a"
      },
      "source": [
        "##### Dataset:\n",
        "\n",
        "Importing the dataset and creating dummy variables for the catgorical variables. As we are going to predict the species of the penguins, the 'species' columns gets assigned as a target variable. Finally, we scale the predictor variables and get the dataset ready for the modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03565b33",
      "metadata": {
        "id": "03565b33"
      },
      "outputs": [],
      "source": [
        "dataset  = pd.read_csv('/penguins_size.csv')\n",
        "dataset=dataset.dropna()\n",
        "df = dataset.copy()\n",
        "target = 'species'\n",
        "encode = ['sex','island']\n",
        "\n",
        "for col in encode:\n",
        "    dummy = pd.get_dummies(df[col], prefix=col)\n",
        "    df = pd.concat([df,dummy], axis=1)\n",
        "    del df[col]\n",
        "\n",
        "target_mapper = {'Adelie':0, 'Gentoo':1, 'Chinstrap':2}\n",
        "def target_encode(val):\n",
        "    return target_mapper[val]\n",
        "\n",
        "df['species'] = df['species'].apply(target_encode)\n",
        "X = df.drop('species', axis=1)\n",
        "y = df['species']\n",
        "# scaling the data\n",
        "from sklearn import preprocessing\n",
        "X = preprocessing.scale(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d0d5667",
      "metadata": {
        "id": "5d0d5667"
      },
      "source": [
        "##### Split the dataset to train and test sets:\n",
        "The dataset is split to 80-20 ratio where the 80% of the dataset will be used to train the model and the rest 20% will be used for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0df88528",
      "metadata": {
        "id": "0df88528"
      },
      "outputs": [],
      "source": [
        "#from sklearn.cross_validation import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d065f766",
      "metadata": {
        "id": "d065f766"
      },
      "source": [
        "##### **XGBoost model:**\n",
        "In order to explore the parameters, we are taking 5 different models as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8330734",
      "metadata": {
        "id": "f8330734"
      },
      "outputs": [],
      "source": [
        "model1 = xgb.XGBClassifier()\n",
        "model2 = xgb.XGBClassifier(max_depth=0)\n",
        "model3 = xgb.XGBClassifier(learning_rate=0.0002)\n",
        "model4 = xgb.XGBClassifier(min_child_weight=24)\n",
        "model5 = xgb.XGBClassifier(gamma=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "872463b2",
      "metadata": {
        "id": "872463b2"
      },
      "source": [
        "###### **Model 1: default parameters**\n",
        "Model 1 has all the default parameters of the XGBoost model and they are shown/printed below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd2dbb5f",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd2dbb5f",
        "outputId": "d2ca415b-74c5-4153-be34-7d8895e681d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBClassifier()\n"
          ]
        }
      ],
      "source": [
        "print(model1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d37a05e8",
      "metadata": {
        "id": "d37a05e8"
      },
      "source": [
        "###### **Model 2: max_depth**\n",
        "The default max_depth for model 2 has been changed to 0 to see how it affects the prediction.  \n",
        "\n",
        "- max_depth [default=6]: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree. exact tree method requires non-zero value. Range: [0,∞]\n",
        "\n",
        "Set parameters for the model 2 are shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c26345",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03c26345",
        "outputId": "c8d124bc-afe8-489a-9012-fb1ab2629c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBClassifier(max_depth=0)\n"
          ]
        }
      ],
      "source": [
        "print(model2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d6bad0e",
      "metadata": {
        "id": "9d6bad0e"
      },
      "source": [
        "###### **Model 3: learning_rate** \n",
        "The default learning_rate for model 3 has been changed to 0.0002 to see how it affects the prediction.  \n",
        "\n",
        "- learning_rate [default=0.3]: Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative. Range: [0,1]\n",
        "\n",
        "Set parameters for the model 3 are shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45adfc1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45adfc1a",
        "outputId": "b55eabea-599e-47f5-d8a6-65b27cf4351a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBClassifier(learning_rate=0.0002)\n"
          ]
        }
      ],
      "source": [
        "print(model3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520e026a",
      "metadata": {
        "id": "520e026a"
      },
      "source": [
        "###### **Model 4: min_child_weight**\n",
        "The default min_child_weight for model 4 has been changed to 24 to see how it affects the prediction.  \n",
        "\n",
        "- min_child_weight [default=1]: Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be. Range: [0,∞]\n",
        "\n",
        "Set parameters for the model 4 are shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f67f23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90f67f23",
        "outputId": "9a44f1d5-8024-4d4c-a898-794635d2d0f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBClassifier(min_child_weight=24)\n"
          ]
        }
      ],
      "source": [
        "print(model4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a77d42e6",
      "metadata": {
        "id": "a77d42e6"
      },
      "source": [
        "##### **Model 5: gamma**\n",
        "The default gamma for model 4 has been changed to 50 to see how it affects the prediction.  \n",
        "\n",
        "- gamma [default=0, alias: min_split_loss]: Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be. Range: [0,∞]\n",
        "\n",
        "Set parameters for the model 5 are shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acbec93f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acbec93f",
        "outputId": "d5772505-98a8-4291-8492-c5a6e1f50752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBClassifier(gamma=50)\n"
          ]
        }
      ],
      "source": [
        "print(model5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "857ca2f0",
      "metadata": {
        "id": "857ca2f0"
      },
      "source": [
        "All models are fitted to the train datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba9b61f",
      "metadata": {
        "id": "8ba9b61f"
      },
      "outputs": [],
      "source": [
        "train_model1 = model1.fit(X_train, y_train)\n",
        "train_model2 = model2.fit(X_train, y_train)\n",
        "train_model3 = model3.fit(X_train, y_train)\n",
        "train_model4 = model4.fit(X_train, y_train)\n",
        "train_model5 = model5.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17c37081",
      "metadata": {
        "id": "17c37081"
      },
      "source": [
        "Species of the penguins were predicted and the accuracy rate of all models were found as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84ae5723",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84ae5723",
        "outputId": "77f294d0-7576-4c1c-f4d3-9e3ea3d0970e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for model 1: 100.00%\n",
            "Accuracy for model 2: 41.79%\n",
            "Accuracy for model 3: 98.51%\n",
            "Accuracy for model 4: 97.01%\n",
            "Accuracy for model 5: 74.63%\n"
          ]
        }
      ],
      "source": [
        "predicted_y1 = train_model1.predict(X_test)\n",
        "predicted_y2 = train_model2.predict(X_test)\n",
        "predicted_y3 = train_model3.predict(X_test)\n",
        "predicted_y4 = train_model4.predict(X_test)\n",
        "predicted_y5 = train_model5.predict(X_test)\n",
        "\n",
        "accuracy1 = accuracy_score(y_test,predicted_y1)\n",
        "accuracy2 = accuracy_score(y_test,predicted_y2)\n",
        "accuracy3 = accuracy_score(y_test,predicted_y3)\n",
        "accuracy4 = accuracy_score(y_test,predicted_y4)\n",
        "accuracy5 = accuracy_score(y_test,predicted_y5)\n",
        "\n",
        "print(\"Accuracy for model 1: %.2f%%\" %(accuracy1*100))\n",
        "print(\"Accuracy for model 2: %.2f%%\" %(accuracy2*100))\n",
        "print(\"Accuracy for model 3: %.2f%%\" %(accuracy3*100))\n",
        "print(\"Accuracy for model 4: %.2f%%\" %(accuracy4*100))\n",
        "print(\"Accuracy for model 5: %.2f%%\" %(accuracy5*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfea626c",
      "metadata": {
        "id": "bfea626c"
      },
      "source": [
        "######**Model 1: default parameters**\n",
        "Model 1 has all the default parameters of the XGBoost model where the defaults are: max_depth=6, learning_rate=0.3, min_child_weight=1, gamma=0. The accuracy for the default parameter is 100%.\n",
        "\n",
        "######**Model 2: max_depth**\n",
        "For the model 2 the default max_depth has been changed to 0 to see how it affects the prediction. From the results we can see that the accuracy has been reduced to 41%. Hence, we can conclude that as we decrease the max_depth the model will perform badly. \n",
        "\n",
        "###### **Model 3: learning_rate**\n",
        "For model 3 the default learning_rate has been changed to 0.0002 to see how it affects the prediction.As the learning rate decreased, the accuracy rate lightly decreased to 98%. The drop in the accuracy has been observed only at the 3rd decimal place values of the learning rate. \n",
        "\n",
        "###### **Model 4: min_child_weight**\n",
        "For model 4 the default min_child_weight has been changed to 24 to see how it affects the prediction. As we see from the results the accuracy has dropped to 95% as the min_child_weight increased.\n",
        "\n",
        "##### **Model 5: gamma**\n",
        "For model 5 the default gamma has been changed to 50 to see how it affects the prediction. The accuracy rate gradually decreased to 73% as the gamma has been increased from 0 to 50."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d5c86d",
      "metadata": {
        "id": "90d5c86d"
      },
      "source": [
        "#### **Comparison:**\n",
        "The accuracy for the AdaBoost (default) model was found to be 93% in the previous practice activity. However, the accuracy of AdaBoost model improved to 99% when the base estimator was changed from decision tree to SVC.\n",
        "\n",
        "But even with such the improvement, XGBoost overperforms AdaBoost model as it has a 100% accuracy. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some links for for more reference and reading:\n",
        "\n",
        "1. https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
        "2. https://xgboost.readthedocs.io/en/stable/parameter.html\n",
        "3. https://blog.paperspace.com/adaboost-optimizer/"
      ],
      "metadata": {
        "id": "6Sch4AC1YvJm"
      },
      "id": "6Sch4AC1YvJm"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "XG Boosting.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}